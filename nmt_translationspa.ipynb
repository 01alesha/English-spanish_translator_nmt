{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4b83eb",
   "metadata": {},
   "source": [
    "### ENGLISH TO SPANISH TRANSLATOR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65275839",
   "metadata": {},
   "source": [
    "#### Loading and Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa774b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, Attention, Concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Loading dataset \n",
    "zip_path = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "data_dir = pathlib.Path(zip_path).parent / \"spa-eng\"\n",
    "text_file = data_dir / \"spa.txt\"\n",
    "with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")[:10000]\n",
    "sentence_pairs = [line.split(\"\\t\") for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1351162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    return sentence.strip()\n",
    "\n",
    "cleaned_pairs = []\n",
    "for eng, spa in sentence_pairs:\n",
    "    eng = preprocess_sentence(eng)\n",
    "    spa = preprocess_sentence(spa)\n",
    "    spa = \"sos \" + spa + \" eos\"\n",
    "    cleaned_pairs.append((eng, spa))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed015d7c",
   "metadata": {},
   "source": [
    "#### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417b0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_texts, spa_texts = zip(*cleaned_pairs)\n",
    "\n",
    "eng_tokenizer = Tokenizer(filters='', lower=True)\n",
    "spa_tokenizer = Tokenizer(filters='', lower=True)\n",
    "eng_tokenizer.fit_on_texts(eng_texts)\n",
    "spa_tokenizer.fit_on_texts(spa_texts)\n",
    "reverse_spa_index = {v: k for k, v in spa_tokenizer.word_index.items()}\n",
    "\n",
    "\n",
    "eng_seq = eng_tokenizer.texts_to_sequences(eng_texts)\n",
    "spa_seq = spa_tokenizer.texts_to_sequences(spa_texts)\n",
    "\n",
    "max_eng_len = max(len(seq) for seq in eng_seq)\n",
    "max_spa_len = max(len(seq) for seq in spa_seq)\n",
    "\n",
    "encoder_input = pad_sequences(eng_seq, maxlen=max_eng_len, padding='post')\n",
    "decoder_input = pad_sequences([seq[:-1] for seq in spa_seq], maxlen=max_spa_len-1, padding='post')\n",
    "decoder_target = pad_sequences([seq[1:] for seq in spa_seq], maxlen=max_spa_len-1, padding='post')\n",
    "\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "embedding_dim = 128\n",
    "latent_dim = 256\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f0975d",
   "metadata": {},
   "source": [
    "#### Model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0da9f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)  [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, None, 128)            294912    ['encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, None, 128)            556800    ['decoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, None, 256),          394240    ['embedding_2[0][0]']         \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, None, 256),          394240    ['embedding_3[0][0]',         \n",
      "                              (None, 256),                           'lstm_2[0][1]',              \n",
      "                              (None, 256)]                           'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " attention_1 (Attention)     (None, None, 256)            0         ['lstm_3[0][0]',              \n",
      "                                                                     'lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, None, 512)            0         ['lstm_3[0][0]',              \n",
      " )                                                                   'attention_1[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 4350)           2231550   ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3871742 (14.77 MB)\n",
      "Trainable params: 3871742 (14.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 98s 386ms/step - loss: 2.3843 - accuracy: 0.6694 - val_loss: 2.0608 - val_accuracy: 0.7020\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 1.5918 - accuracy: 0.7544 - val_loss: 1.8776 - val_accuracy: 0.7318\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 70s 281ms/step - loss: 1.3861 - accuracy: 0.7786 - val_loss: 1.7701 - val_accuracy: 0.7468\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 1.2320 - accuracy: 0.7941 - val_loss: 1.7085 - val_accuracy: 0.7573\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 53s 214ms/step - loss: 1.0935 - accuracy: 0.8075 - val_loss: 1.6648 - val_accuracy: 0.7653\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 53s 213ms/step - loss: 0.9617 - accuracy: 0.8199 - val_loss: 1.6535 - val_accuracy: 0.7741\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 51s 206ms/step - loss: 0.8387 - accuracy: 0.8322 - val_loss: 1.6510 - val_accuracy: 0.7795\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 0.7283 - accuracy: 0.8450 - val_loss: 1.6511 - val_accuracy: 0.7826\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 52s 210ms/step - loss: 0.6293 - accuracy: 0.8580 - val_loss: 1.6691 - val_accuracy: 0.7867\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 53s 210ms/step - loss: 0.5449 - accuracy: 0.8714 - val_loss: 1.6669 - val_accuracy: 0.7885\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.4705 - accuracy: 0.8842 - val_loss: 1.6717 - val_accuracy: 0.7920\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 51s 205ms/step - loss: 0.4067 - accuracy: 0.8961 - val_loss: 1.6896 - val_accuracy: 0.7926\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.3552 - accuracy: 0.9072 - val_loss: 1.7063 - val_accuracy: 0.7927\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 52s 210ms/step - loss: 0.3099 - accuracy: 0.9173 - val_loss: 1.7324 - val_accuracy: 0.7955\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 52s 209ms/step - loss: 0.2726 - accuracy: 0.9253 - val_loss: 1.7424 - val_accuracy: 0.7968\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 0.2402 - accuracy: 0.9335 - val_loss: 1.7482 - val_accuracy: 0.7963\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.2161 - accuracy: 0.9390 - val_loss: 1.7724 - val_accuracy: 0.7973\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 52s 209ms/step - loss: 0.1949 - accuracy: 0.9449 - val_loss: 1.7914 - val_accuracy: 0.7975\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 63s 253ms/step - loss: 0.1765 - accuracy: 0.9491 - val_loss: 1.8024 - val_accuracy: 0.7970\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 78s 314ms/step - loss: 0.1625 - accuracy: 0.9519 - val_loss: 1.8160 - val_accuracy: 0.7965\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 63s 251ms/step - loss: 0.1508 - accuracy: 0.9549 - val_loss: 1.8261 - val_accuracy: 0.7988\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 0.1415 - accuracy: 0.9573 - val_loss: 1.8470 - val_accuracy: 0.7977\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.1338 - accuracy: 0.9592 - val_loss: 1.8536 - val_accuracy: 0.7980\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 56s 225ms/step - loss: 0.1261 - accuracy: 0.9610 - val_loss: 1.8631 - val_accuracy: 0.7981\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 0.1205 - accuracy: 0.9619 - val_loss: 1.8633 - val_accuracy: 0.7972\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
    "encoder_emb = Embedding(eng_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(encoder_emb)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
    "decoder_emb = Embedding(spa_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True)(decoder_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention\n",
    "attention = Attention()\n",
    "context_vector = attention([decoder_lstm, encoder_lstm])\n",
    "decoder_concat = Concatenate(axis=-1)([decoder_lstm, context_vector])\n",
    "\n",
    "# Output layer\n",
    "decoder_outputs = Dense(spa_vocab_size, activation='softmax')(decoder_concat)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.fit([encoder_input, decoder_input], decoder_target,\n",
    "          batch_size=32,\n",
    "          epochs=25,\n",
    "          validation_split=0.2)\n",
    "\n",
    "model.save(\"final_nmt_model.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b694ff5",
   "metadata": {},
   "source": [
    "#### Inference setup and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93443a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "\n",
    "encoder_inf_inputs = model.get_layer('encoder_input').input\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.get_layer('lstm_2').output\n",
    "encoder_model = Model(encoder_inf_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_hidden_state_input = Input(shape=(None, 256))  \n",
    "\n",
    "decoder_inf_inputs = Input(shape=(1,))\n",
    "\n",
    "\n",
    "dec_emb_layer = model.get_layer('embedding_3')\n",
    "decoder_lstm_layer = model.get_layer('lstm_3')\n",
    "attention_layer = model.get_layer('attention_1')\n",
    "concat_layer = model.get_layer('concatenate_1')\n",
    "dense_layer = model.get_layer('dense_1')\n",
    "\n",
    "# Embedding\n",
    "dec_emb_inf = dec_emb_layer(decoder_inf_inputs)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_outputs, state_h, state_c = decoder_lstm_layer(\n",
    "    dec_emb_inf, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "\n",
    "# Attention\n",
    "attn_out_inf = attention_layer([decoder_outputs, decoder_hidden_state_input])\n",
    "decoder_concat_inf = concat_layer([decoder_outputs, attn_out_inf])\n",
    "\n",
    "# Final output layer\n",
    "decoder_outputs_final = dense_layer(decoder_concat_inf)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_model = Model(\n",
    "    [decoder_inf_inputs, decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs_final, state_h, state_c]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5393ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    enc_outs, h, c = encoder_model.predict(input_seq)\n",
    "    target_seq = np.array([[spa_tokenizer.word_index['sos']]])\n",
    "    decoded_sentence = ''\n",
    "    stop_condition = False\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq, enc_outs, h, c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_spa_index.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_word == 'eos' or len(decoded_sentence.split()) > max_spa_len:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        target_seq = np.array([[sampled_token_index]])\n",
    "\n",
    "    return decoded_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87507c9",
   "metadata": {},
   "source": [
    "#### Testing and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a681c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 490ms/step\n",
      "1/1 [==============================] - 1s 767ms/step\n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "English: who are you\n",
      "Spanish: aabe .\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "English: how was your day\n",
      "Spanish: no hay un cerdo .\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "English: what are you doing ?\n",
      "Spanish: ¿ c mo est n ?\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
    "    padded = pad_sequences(seq, maxlen=max_eng_len, padding='post')\n",
    "    translation = decode_sequence(padded)\n",
    "    print(f\"English: {sentence}\")\n",
    "    print(f\"Spanish: {translation}\")\n",
    "\n",
    "translate(\"who are you\")\n",
    "translate(\"how was your day\")\n",
    "translate(\"What are you doing?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14d172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "model.save(\"spanish_translation_model.keras\")\n",
    "\n",
    "with open(\"eng_tokenizer.json\", \"w\") as f:\n",
    "    f.write(eng_tokenizer.to_json())\n",
    "\n",
    "with open(\"spa_tokenizer.json\", \"w\") as f:\n",
    "    f.write(spa_tokenizer.to_json())\n",
    "\n",
    "reverse_spa_index = {v: k for k, v in spa_tokenizer.word_index.items()}\n",
    "with open(\"reverse_spa_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reverse_spa_index, f)\n",
    "\n",
    "\n",
    "with open(\"seq_lengths.json\", \"w\") as f:\n",
    "    json.dump({\"max_eng_len\": max_eng_len, \"max_spa_len\": max_spa_len}, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
